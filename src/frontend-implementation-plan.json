{
  "kind": "implementation_plan",
  "version": "1.0",
  "title": "Resilient CSV/JSON dataset uploads mapped to per-row documents",
  "requirements": [
    {
      "id": "REQ-22",
      "summary": "Accept CSV/JSON dataset uploads with the exact required schema and validate required `text` before attempting any uploads.",
      "acceptanceCriteria": [
        "Uploading a .csv file with a header row containing exactly the columns `ID,Date,Region,Source,User,text,Aspect_Category,Keywords_Extracted` (case-insensitive, whitespace-tolerant) succeeds.",
        "Uploading a .json file containing an array of objects with keys matching `ID,Date,Region,Source,User,text,Aspect_Category,Keywords_Extracted` (case-insensitive) succeeds.",
        "If a CSV/JSON upload is missing the required `text` field/column, the UI shows a clear error message and does not attempt to upload any documents.",
        "If a CSV/JSON upload has the required schema but contains some rows with empty `text`, those rows are skipped and the upload still succeeds for the remaining valid rows (with a clear warning message indicating how many rows were skipped)."
      ],
      "file_operations": [
        {
          "path": "frontend/src/lib/datasetIngestion.ts",
          "operation": "create",
          "description": "Add dataset parsing + schema validation utilities for .csv/.json uploads: normalize header/keys (case-insensitive, whitespace-tolerant), enforce required columns list, detect missing required `text`, extract per-row `text`, and compute skipped-empty-text counts. Ensure parser errors surface as typed/structured errors for UI display."
        },
        {
          "path": "frontend/src/pages/Dashboard.tsx",
          "operation": "modify",
          "description": "Update the file upload handler to branch by file type: keep .txt behavior as single-document upload, and for .csv/.json use the new dataset ingestion utilities to validate the schema (including `text` presence) before starting any uploads; on validation failure, show a clear error message and abort without calling upload mutations."
        }
      ]
    },
    {
      "id": "REQ-23",
      "summary": "For valid dataset uploads, upload each row as a separate document using the rowâ€™s `text` value as Document.content and refresh visualizations automatically.",
      "acceptanceCriteria": [
        "A CSV with N valid rows (non-empty `text`) results in N new documents in `getAllDocuments()` (not 1 document containing the whole file).",
        "A JSON array with N valid entries (non-empty `text`) results in N new documents in `getAllDocuments()`.",
        "After a successful dataset upload, Dashboard visualizations update based on the new documents (no manual refresh needed)."
      ],
      "file_operations": [
        {
          "path": "frontend/src/hooks/useQueries.ts",
          "operation": "modify",
          "description": "Add a batch upload mutation (e.g., `useUploadDocumentsBatch`) that accepts an array of document contents and uploads them as multiple `uploadDocument` calls, with optional progress callbacks. Ensure React Query invalidations for documents/analytics occur once at the end so Dashboard visualizations refresh automatically without manual reload."
        },
        {
          "path": "frontend/src/pages/Dashboard.tsx",
          "operation": "modify",
          "description": "Wire dataset parsing output (array of valid `text` rows) into the new batch upload mutation so each row becomes its own backend document; ensure empty `text` rows are not uploaded and success navigations (if any) only occur after the batch completes."
        }
      ]
    },
    {
      "id": "REQ-24",
      "summary": "Improve dataset upload UX with clear parse/validation errors, batch progress reporting, and per-row success/failure summaries while preserving .txt single-document uploads.",
      "acceptanceCriteria": [
        "If parsing fails (e.g., malformed CSV), the UI shows a clear error message describing that the file could not be parsed.",
        "If uploading one or more rows fails during a multi-row upload, the UI reports how many rows succeeded and how many failed, and the operation does not silently fail.",
        "The file input allows `.txt`, `.csv`, `.json` as before; `.txt` continues to upload as a single document (current behavior preserved)."
      ],
      "file_operations": [
        {
          "path": "frontend/src/components/DatasetUploadStatus.tsx",
          "operation": "create",
          "description": "Create a presentational component to display dataset upload state (parsing/validating/uploading/done), including total rows, uploaded count, failed count, skipped-empty-text count, and the latest error message if present; use existing shadcn-ui components already in the project (do not modify `frontend/src/components/ui/*`). Verify the component's usage instructions before implementing."
        },
        {
          "path": "frontend/src/pages/Dashboard.tsx",
          "operation": "modify",
          "description": "Add resilient UI state management around uploads: show parse/validation errors from dataset ingestion, show live upload progress during multi-row uploads, and on completion show a clear summary (succeeded/failed/skipped). Keep `.txt` flow unchanged (single upload using existing mutation) and ensure `.csv/.json` failures do not silently fail."
        },
        {
          "path": "frontend/src/lib/datasetIngestion.ts",
          "operation": "modify",
          "description": "Extend ingestion utilities to produce debuggable error messages for malformed CSV/JSON and schema mismatches, plus row-level validation results that enable UI to summarize skipped/invalid rows without attempting uploads when schema is invalid."
        }
      ]
    }
  ]
}