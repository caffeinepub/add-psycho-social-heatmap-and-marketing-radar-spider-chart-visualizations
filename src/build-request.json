{
  "kind": "build_request",
  "title": "Accept CSV/JSON uploads with dataset columns ID, Date, Region, Source, User, text, Aspect_Category, Keywords_Extracted",
  "priority": "high",
  "requirements": [
    {
      "id": "REQ-22",
      "text": "Update the file-upload ingestion flow so users can upload datasets whose schema is exactly: `ID, Date, Region, Source, User, text, Aspect_Category, Keywords_Extracted`, without the upload failing.",
      "target": "frontend",
      "source": {
        "messageIds": [
          "recent-messages"
        ],
        "quotes": [
          "datasetnya berikut ini untuk dijadikan format dalam upload file: ID,Date,Region,Source,User,text,Aspect_Category,Keywords_Extracted"
        ]
      },
      "acceptanceCriteria": [
        "Uploading a .csv file with a header row containing exactly the columns `ID,Date,Region,Source,User,text,Aspect_Category,Keywords_Extracted` (case-insensitive, whitespace-tolerant) succeeds.",
        "Uploading a .json file containing an array of objects with keys matching `ID,Date,Region,Source,User,text,Aspect_Category,Keywords_Extracted` (case-insensitive) succeeds.",
        "If a CSV/JSON upload is missing the required `text` field/column, the UI shows a clear error message and does not attempt to upload any documents.",
        "If a CSV/JSON upload has the required schema but contains some rows with empty `text`, those rows are skipped and the upload still succeeds for the remaining valid rows (with a clear warning message indicating how many rows were skipped)."
      ]
    },
    {
      "id": "REQ-23",
      "text": "For dataset uploads (.csv/.json) that follow the specified schema, parse the file into multiple documents and upload each row as a separate backend document using the rowâ€™s `text` value as `Document.content`.",
      "target": "frontend",
      "source": {
        "messageIds": [
          "recent-messages"
        ],
        "quotes": [
          "datasetnya berikut ini untuk dijadikan format dalam upload file: ID,Date,Region,Source,User,text,Aspect_Category,Keywords_Extracted"
        ]
      },
      "acceptanceCriteria": [
        "A CSV with N valid rows (non-empty `text`) results in N new documents in `getAllDocuments()` (not 1 document containing the whole file).",
        "A JSON array with N valid entries (non-empty `text`) results in N new documents in `getAllDocuments()`.",
        "After a successful dataset upload, Dashboard visualizations update based on the new documents (no manual refresh needed)."
      ]
    },
    {
      "id": "REQ-24",
      "text": "Make the upload experience resilient and debuggable by reporting parse/validation errors and upload progress/failures clearly in the UI.",
      "target": "frontend",
      "source": {
        "messageIds": [
          "recent-messages"
        ],
        "quotes": [
          "kenapa gagal upload file? tolong perbaiki agar tidak gagal proses upload file",
          "pastikan perbaikan pada upload file apakah sudah diperbaiki?"
        ]
      },
      "acceptanceCriteria": [
        "If parsing fails (e.g., malformed CSV), the UI shows a clear error message describing that the file could not be parsed.",
        "If uploading one or more rows fails during a multi-row upload, the UI reports how many rows succeeded and how many failed, and the operation does not silently fail.",
        "The file input allows `.txt`, `.csv`, `.json` as before; `.txt` continues to upload as a single document (current behavior preserved)."
      ]
    }
  ],
  "constraints": [
    "Do not modify any files under `frontend/src/components/ui` or other immutable paths listed in SYSTEM_CONTEXT.",
    "Keep the backend as a single Motoko actor in `backend/main.mo`; do not introduce additional backend services or external databases.",
    "Prefer frontend-only schema mapping/parsing; do not change the backend `Document` type/schema unless strictly required for the upload to succeed."
  ],
  "nonGoals": [
    "Implementing a full data warehouse or advanced ETL pipeline for uploaded datasets.",
    "Adding real-time upload progress via websockets or push notifications.",
    "Changing existing analytics logic beyond what is necessary to ensure uploaded dataset rows become documents and charts can render from them."
  ],
  "imageRequirements": {
    "required": [],
    "edits": []
  }
}